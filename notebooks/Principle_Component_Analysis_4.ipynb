{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "\n",
        "def configure_logging(level=logging.INFO, log_path=None):\n",
        "    if log_path is None:\n",
        "        log_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'logs')\n",
        "    if not os.path.exists(log_path):\n",
        "        os.mkdir(log_path)\n",
        "\n",
        "    log_file = os.path.join(log_path, f\"{os.path.dirname(os.path.realpath(__file__)).split(os.sep)[-1]}.log\")\n",
        "    if level == logging.INFO or logging.NOTSET:\n",
        "        logging.basicConfig(\n",
        "            level=level,\n",
        "            format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "    elif level == logging.DEBUG or level == logging.ERROR:\n",
        "        logging.basicConfig(\n",
        "            level=level,\n",
        "            format=\"%(asctime)s %(filename)s function:%(funcName)s()\\t[%(levelname)s] %(message)s\",\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "def str_or_none(value):\n",
        "    return value if value is None else str(value)\n",
        "\n",
        "\n",
        "def main(rootpath, loader):\n",
        "    # Bring in data to pre-process\n",
        "    if loader == 'DataFile':\n",
        "        datafile = os.path.join(rootpath, 'data', 'Atlanta Prices.csv')\n",
        "        df = pd.read_csv(datafile)\n",
        "    else:\n",
        "        pickle_file = os.path.join(rootpath, 'snapshots', 'preprocessed.pkl')\n",
        "        with open(pickle_file, 'rb') as file:\n",
        "            df = pickle.load(file)\n",
        "\n",
        "    # Step 1: One-hot encode non-numeric columns\n",
        "    logging.debug(\"Converting non-numerics to one-hot encoded columns\")\n",
        "    non_numeric_columns = df.select_dtypes(exclude=['int', 'float']).columns\n",
        "    logging.debug(f\"Non-numeric columns: {non_numeric_columns}\")\n",
        "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "    encoded_columns = pd.DataFrame(one_hot_encoder.fit_transform(df[non_numeric_columns]))\n",
        "    encoded_columns.columns = one_hot_encoder.get_feature_names_out(non_numeric_columns)\n",
        "    df_PCA = pd.concat([df.drop(columns=non_numeric_columns), encoded_columns], axis=1)\n",
        "\n",
        "    # Step 2: Scale the data\n",
        "    logging.debug(\"Scaling the data\")\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df_PCA)\n",
        "\n",
        "    # Step 3: Perform PCA\n",
        "    # Specify what columns need to stay\n",
        "    logging.debug(\"Performing PCA\")\n",
        "    num_components = 24  # Change this to your desired number of components\n",
        "    pca = PCA(n_components=num_components)\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # Now, pca_result contains the transformed data after PCA\n",
        "    # Convert to PCA dataframe\n",
        "    logging.debug(\"Applying PCA with variance ratio threshold\")\n",
        "    pca_dataframe = pd.DataFrame(data=pca_result, columns=[f'PC{i + 1}' for i in range(num_components)])\n",
        "    logging.debug(pca_dataframe.head())\n",
        "\n",
        "    # Determine the number of components based on a threshold (e.g., 95% variance explained)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
        "    num_components = sum(cumulative_variance_ratio >= 0.95)\n",
        "    logging.debug(f\"PCA results from number of components: {num_components}\")\n",
        "    # Apply PCA with the determined number of components\n",
        "    pca = PCA(n_components=num_components)\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # Save the PCA model\n",
        "    logging.debug(\"Saving PCA model\")\n",
        "    with open(os.path.join(rootpath, 'snapshots', 'pca.pkl'), 'wb') as f:\n",
        "        pickle.dump(pca, f)\n",
        "\n",
        "    # Now, pca_result contains the transformed data after PCA\n",
        "    pca_dataframe = pd.DataFrame(data=pca_result, columns=[f'PC{i + 1}' for i in range(num_components)])\n",
        "\n",
        "    # Print the resulting DataFrame and explained variance\n",
        "    logging.debug(pca_dataframe.head())\n",
        "    logging.info(f\"PCA results from number of components: {num_components}\")\n",
        "    logging.info(f\"Cumulative explained variance: {cumulative_variance_ratio[num_components - 1]:.2%}\")\n",
        "\n",
        "    # Plot individual explained variance\n",
        "    logging.debug(\"Plotting PCA results\")\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "    plt.title('Explained Variance by Principal Component')\n",
        "    plt.savefig(os.path.join(rootpath, 'outputs', 'PCA - Explained Variance.jpg'), format='jpeg')\n",
        "\n",
        "    # Plot cumulative explained variance\n",
        "    logging.debug(\"Plotting cumulative explained variance\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
        "    plt.xlabel('Number of Principal Components')\n",
        "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "    plt.title('Cumulative Explained Variance')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(rootpath, 'outputs', 'PCA - Cumulative Explained Variance.jpg'), format='jpeg')\n",
        "\n",
        "    # Plot reduced-dimensional scatter plot\n",
        "    # logging.debug(\"Plotting reduced-dimensional scatter plot\")\n",
        "    # logging.info(\"Shape of pca_result:\", pca_result.shape)\n",
        "    # if len(pca_result.shape) == 1:\n",
        "    #     pca_result = pca_result.reshape(-1, 1)\n",
        "    # try:\n",
        "    #     plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
        "    #     plt.xlabel('Principal Component 1')\n",
        "    #     plt.ylabel('Principal Component 2')\n",
        "    #     plt.title('PCA: Reduced-dimensional Scatter Plot')\n",
        "    #     plt.savefig(os.path.join(rootpath, 'outputs', 'PCA - Reduced dimensional Scatter Plot.jpg'), format='jpeg')\n",
        "\n",
        "    logging.info(\"PCA completed successfully\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Principal Component Analysis')\n",
        "    parser.add_argument('--root_path', type=str_or_none, help='Root path of the project. Should have the snapshots/data folder', default=None)\n",
        "    parser.add_argument('--loader', type=str_or_none, help='Type of loader to use. Literals \"DataFile\" or \"Memory\".Default - Memory', default=\"Memory\")\n",
        "    args = parser.parse_args()\n",
        "    if args.root_path is None:\n",
        "        args.root_path = os.path.dirname(__file__)\n",
        "    configure_logging(logging.DEBUG, os.path.join(args.root_path, 'logs'))\n",
        "    if ['DataFile', 'Memory'] not in args.loader:\n",
        "        logging.warning(\"Invalid loader. Valid loaders are 'DataFile' or 'Memory'. Defaulting to 'Memory'\")\n",
        "    args.loader = 'Memory'\n",
        "    try:\n",
        "        main(args.root_path, args.loader)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Keyboard Interrupt\")\n",
        "        exit(-1)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        exit(1)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}